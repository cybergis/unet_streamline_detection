{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJj7NuZlyZGJ"
   },
   "source": [
    "<h1 style=\"text-align:center;line-height:1.5em;font-size:30px;\">Data and Scripts <br>for Hydrological Streamline Detection Using a U-net model</h1>\n",
    "<p style=\"text-align:center;font-size:12px;\">\n",
    "$Zewei$ $Xu^{1,2,4}$; $Nattapon$ $Jaroenchai^{1,2,4}$; $Arpan$ $Man$ $Sainju^{3,5}$; $Li$ $Chen^{1,2,4}$; $Zhiyu$ $Li^{1,2,4}$; $Larry$ $Stanislawski^{6}$; $Ethan$ $Shavers^{6}$; $Bin$ $Su^{1,2,4}$; $Zhe$ $Jiang^{3,5}$; $Shaowen$ $Wang^{1,2,4}$\n",
    "</p>\n",
    "<p style=\"text-align:center;font-size:12px;\">\n",
    "$^{1}$$CyberGIS$ $Center$ $for$ $Advanced$ $Digital$ $and$ $Spatial$ $Studies$<br>\n",
    "$^{2}$$Department$ $of$ $Geography$ $and$ $Geographic$ $Information$ $Science$<br>\n",
    "$^{3}$$Department$ $of$ $Computer$ $Science$<br>\n",
    "$^{4}$$University$ $of$ $Illinois$ $at$ $Urbana-Champaign$<br>\n",
    "$^{5}$$University$ $of$ $Alabama$<br>\n",
    "$^{6}$$U.S.$ $Geological$ $Survey$<br>\n",
    "$Corresponding$ $Author:$ $zeweixu2@illinois.edu$\n",
    "    </p>\n",
    "\n",
    "---\n",
    "    \n",
    "**Notebook Structure:**\n",
    "- [Introduction](1_introduction.ipynb)\n",
    "- Codes\n",
    " - [Data Preprocessing](2.1_Code_Data_Preprocessing.ipynb)\n",
    " - [Model Training](2.2_Code_Model_Training.ipynb)\n",
    " - [Interpret the Result](2.3_Code_Interpret_the_Result%20.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRgnHKrhyZGM"
   },
   "source": [
    "---\n",
    "\n",
    "### Data Preprocessing \n",
    "\n",
    "This part of code will generate the samples for training and validation process. \n",
    "\n",
    "In this research, the training and validatin are generated in 4 secenarios base on the parameter m in the block below,\n",
    "\n",
    "1. scenario 1 (m=\"n\"): the upper half is used to generate trainging and validation samples.\n",
    "2. scenario 2 (m=\"n2\"): the lower half is used to generate trainging and validation samples.\n",
    "3. scenario 3 (m=\"v\"): the left half is used to generate trainging and validation samples.\n",
    "4. scenario 4 (m=\"v2\"): the right half is used to generate trainging and validation samples.\n",
    "\n",
    "The code will generate 2000 samples then devide the sample into training (2/3) and valiation (1/3) sample sets. Then the data and label of the data will be save to .npy files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3yCBlsYYN5O",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --user imgaug\n",
    "!pip install --user intervaltree\n",
    "!pip install --user numpy --upgrade\n",
    "root=\"/home/jovyan/shared_data/data/unet_streamline_detection/\"\n",
    "save_root=\"/home/jovyan/work/unet_streamline_detection/\"\n",
    "\n",
    "# n/n2: upper_half_as_train_validation/lower_half_as_train_validation, \n",
    "# v/v2:left_half_as_train_validation/right_half_as_train_validation\n",
    "m = 'v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Samples generation\n",
    "\n",
    "First we extract the training and validation samples from the reference dataset. We can only sample small sample size due to the size of the dataset. Therefore, we must augment the trainging samples to increase the training sample size and generalize the smaples.\n",
    "\n",
    "The code below shows the augmentation steps usign the pre-sampling patches in the \"train_patches_top-left_\\*.npy\". The results are saved to train_data_aug\\*.npy and train_label_aug\\*.npy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 184209,
     "status": "ok",
     "timestamp": 1583959869676,
     "user": {
      "displayName": "nattapon jaroenchai",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj9i_TMlYzv27Vkas3v5qWkW7ZdTBQme3RVKqYp7c4=s64",
      "userId": "17092454241854925654"
     },
     "user_tz": 300
    },
    "id": "gBoLpQE3dGe_",
    "outputId": "a725a0de-6d38-40af-dbbd-e5725e353efb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data complete!\n",
      "augmentation saved!\n",
      "Training data augmentation complete!\n"
     ]
    }
   ],
   "source": [
    "# Training data augmentation\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "# reference array\n",
    "reference = np.load(root+'data/reference.npy')\n",
    "# total prediction feature maps\n",
    "total = np.load(root+'data/total.npy')\n",
    "# Top-left coordinates of training patches\n",
    "trainlo = np.load(root+'data/train_patches_top-left'+m+'.npy')\n",
    "# add 200 buffer pixels to each patch\n",
    "\n",
    "print(\"Load data complete!\")\n",
    "\n",
    "pad = 200\n",
    "trainlo[:,0] += pad\n",
    "trainlo[:,1] += pad\n",
    "depth = total.shape[2]\n",
    "reference = np.pad(reference,(pad,pad),'symmetric')\n",
    "for i in range(depth):\n",
    "    temp = np.pad(total[:,:,i],(pad,pad),'symmetric')[:,:,np.newaxis]\n",
    "    if i == 0:\n",
    "        totaln = temp\n",
    "    else:\n",
    "        totaln = np.concatenate((totaln,temp),axis = 2)\n",
    "def process(lpathc,lref):\n",
    "    # rotate a random degree between -50 and 130\n",
    "    lpathc = np.concatenate((lpathc,lref[:,:,np.newaxis]),axis = 2)\n",
    "    rotate = iaa.Affine(rotate=(-50, 130))\n",
    "    image1 = rotate.augment_image(lpathc)\n",
    "    # rotate a random degree between 230 and 310\n",
    "    rotate = iaa.Affine(rotate=(230, 310))\n",
    "    image2 = rotate.augment_image(lpathc)\n",
    "\n",
    "    #Scale a random ratio between 0.3 and 0.6\n",
    "    scale = iaa.Affine(scale={\"x\": (0.3, 0.6), \"y\": (0.3, 0.6)})\n",
    "    image3 = scale.augment_image(lpathc)\n",
    "\n",
    "    #Scale a random ratio between 1.5 and 2.0\n",
    "    scale = iaa.Affine(scale={\"x\": (1.5, 2.0), \"y\": (1.5, 2.0)})\n",
    "    image4 = scale.augment_image(lpathc)\n",
    "    \n",
    "    # shear a random degree between -30 and 30\n",
    "    shear = iaa.Affine(shear=(-30, 30))\n",
    "    image5 = shear.augment_image(lpathc)    \n",
    "    \n",
    "    # flip horizontally\n",
    "    flip = iaa.Fliplr(1.0)\n",
    "    image6 = flip.augment_image(lpathc)  \n",
    "    \n",
    "    # Add Guassian noises\n",
    "    #gua = iaa.AdditiveGaussianNoise(scale=(10, 20))\n",
    "    #image6 = gua.augment_image(lpathc)\n",
    "    #ref6 = gua.augment_image(lref)  \n",
    "    oii = []\n",
    "    orr = []\n",
    "    for i in [image1,image2,image3,image4,image5,image6]:\n",
    "        oii.append(i[pad:(pad+224),pad:(pad+224),:-1])\n",
    "        orr.append(i[pad:(pad+224),pad:(pad+224),-1])\n",
    "    return [oii,orr]\n",
    "\n",
    "# Concatenate augmented training data based on different types of augmentations\n",
    "pc = 0\n",
    "train_data_aug = []\n",
    "for i in range(len(trainlo)):\n",
    "    lo = trainlo[i]\n",
    "    lpatch = totaln[(lo[0]-pad):(lo[0]+224+pad),(lo[1]-pad):(lo[1]+224+pad),:]\n",
    "    lref = reference[(lo[0]-pad):(lo[0]+224+pad),(lo[1]-pad):(lo[1]+224+pad)]\n",
    "    if len(train_data_aug) == 0:\n",
    "        train_data_aug = lpatch[pad:(-pad),pad:(-pad),:][np.newaxis,:,:,:]\n",
    "        train_label_aug = lref[pad:(-pad),pad:(-pad)][np.newaxis,:,:]\n",
    "    else:\n",
    "        train_data_aug = np.concatenate((train_data_aug,lpatch[pad:(-pad),pad:(-pad),:][np.newaxis,:,:,:]),axis = 0)\n",
    "        train_label_aug = np.concatenate((train_label_aug,lref[pad:(-pad),pad:(-pad)][np.newaxis,:,:]),axis = 0)\n",
    "    [reim,rere] = process(lpatch,lref)\n",
    "    for j in range(6):\n",
    "        train_data_aug = np.concatenate((train_data_aug,reim[j][np.newaxis,:,:,:]),axis = 0)\n",
    "        train_label_aug = np.concatenate((train_label_aug,rere[j][np.newaxis,:,:]),axis = 0)\n",
    "    if i%30 == 0:\n",
    "        np.save(save_root+'data/gen/train_data_augP'+str(pc)+'.npy',train_data_aug)\n",
    "        np.save(save_root+'data/gen/train_label_augP'+str(pc)+'.npy',train_label_aug)\n",
    "        train_data_aug = []\n",
    "        train_label_aug = []\n",
    "        pc+=1\n",
    "        \n",
    "# store training data after different types of augmentations\n",
    "np.save(save_root+'data/gen/train_data_augP'+str(pc)+'.npy',train_data_aug)\n",
    "np.save(save_root+'data/gen/train_label_augP'+str(pc)+'.npy',train_label_aug)\n",
    "\n",
    "print(\"augmentation saved!\")\n",
    "\n",
    "# Concatenate the training data of different augmentations\n",
    "for i in range(pc+1):\n",
    "    temp = np.load(save_root+'data/gen/train_data_augP'+str(i)+'.npy')\n",
    "    templ = np.load(save_root+'data/gen/train_label_augP'+str(i)+'.npy')\n",
    "    if i == 0:\n",
    "        fdata = temp\n",
    "        fl = templ\n",
    "    else:\n",
    "        fdata = np.concatenate((fdata,temp),axis = 0)\n",
    "        fl = np.concatenate((fl,templ),axis = 0)\n",
    "\n",
    "# remove unnesessary intermediate files\n",
    "#os.system('rm /content/drive/My Drive/USGS/Notebooks/data/train_*_augP*.npy')\n",
    "# Shuffle the finalized file and save as .npy\n",
    "rand = np.arange(len(fdata))\n",
    "np.random.shuffle(rand)\n",
    "train_data_aug = fdata[rand]\n",
    "train_label_aug = fl[rand]\n",
    "np.save(save_root+'data/gen/train_data_aug'+m+'.npy',train_data_aug)\n",
    "np.save(save_root+'data/gen/train_label_aug'+m+'.npy',train_label_aug[:,:,:,np.newaxis])\n",
    "\n",
    "print(\"Training data augmentation complete!\")\n",
    "###### visualization ########\n",
    "#    import pdb\n",
    "#    pdb.set_trace() \n",
    "#    plt.imshow(lref[pad:(-pad),pad:(-pad)])\n",
    "#    plt.show()\n",
    "#    plt.imshow(lpatch[:,:,0][pad:(-pad),pad:(-pad)])\n",
    "#    plt.show()    \n",
    "#    aug = ['rotate1','rotate2','scale1','scale2','shear','flip']\n",
    "#    for i in range(6):\n",
    "#        print(aug[i])\n",
    "#        plt.imshow(rere[i])\n",
    "#        plt.show()\n",
    "#        plt.imshow(reim[i][:,:,0])\n",
    "#        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w7QSRH-6yZGd"
   },
   "source": [
    "---\n",
    "### Generate testing samples\n",
    "We genrate the testing samples using moving window method that covers entire part left from the training and validation samples. \n",
    "\n",
    "*Note: running this part may consume large memory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 56150,
     "status": "ok",
     "timestamp": 1583959199784,
     "user": {
      "displayName": "nattapon jaroenchai",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj9i_TMlYzv27Vkas3v5qWkW7ZdTBQme3RVKqYp7c4=s64",
      "userId": "17092454241854925654"
     },
     "user_tz": 300
    },
    "id": "DgHs_6qvIv2p",
    "outputId": "a384fa2b-9af2-47cb-afa9-41a6031f0c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: Data Loading!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "root = \"./\"\n",
    "# stream/non-stream sample size\n",
    "\n",
    "size = 2000 # number of samples \n",
    "patch_size = 224 #patch size of each sample\n",
    "\n",
    "#Total data dimension: 3981*2640\n",
    "mask = np.load(root+'data/mask.npy')\n",
    "totaldata = np.load(root+'data/total.npy')\n",
    "totaldata = np.concatenate((totaldata,mask[:,:,np.newaxis]),axis = 2)\n",
    "label = np.load(root+'data/reference.npy')\n",
    "\n",
    "print('Completed: Data Loading!')\n",
    "# buffer size\n",
    "buf = 30\n",
    "it = 'full'\n",
    "# Image dimension\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 224\n",
    "# moving window size = image_dimension - 2*buffer_size\n",
    "mw = IMG_WIDTH - buf*2\n",
    "\n",
    "# Number of trainig channels\n",
    "# Adding padding for moving window\n",
    "totalnew = np.pad(totaldata, (buf, buf), 'symmetric')\n",
    "totalnew = totalnew[:,:,buf:(buf+5)]\n",
    "dim = totaldata.shape[:2]\n",
    "\n",
    "# number of patch rows\n",
    "numr = dim[0]//(IMG_WIDTH - buf*2)#224\n",
    "# number of patch columns\n",
    "print('rows:'+str(numr))\n",
    "numc = dim[1]//(IMG_WIDTH - buf*2)#224\n",
    "print('columns:'+str(numc))\n",
    "\n",
    "# Splitting the total data into patches \n",
    "count = 0\n",
    "for i in range(numr):\n",
    "\tfor j in range(numc):\n",
    "\t\tcount += 1\n",
    "\t\ttemp = totalnew[i*mw:(i*mw+224),j*mw:(j*mw+224),:][np.newaxis,:,:,:]\n",
    "\t\tif count == 1:\n",
    "\t\t\ttotal = temp#[:,:,:,:-1]\n",
    "\t\telse:\n",
    "\t\t\ttotal = np.concatenate((total, temp),axis = 0)\n",
    "# Save the total dataset\n",
    "np.save(save_root+'data/gen/prediction_data.npy',total)\n",
    "print(\"Testing moving window is generate!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "2.1_Code_Data_Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
